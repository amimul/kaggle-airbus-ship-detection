{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System paths\n",
    "root_dir = \"../dataset/\"\n",
    "trainset_dir = os.path.join(root_dir, \"train_v2\")\n",
    "testset_dir = os.path.join(root_dir, \"test_v2\")\n",
    "train_rle = os.path.join(root_dir, \"train_ship_segmentations_v2.csv\")\n",
    "\n",
    "df = pd.read_csv(train_rle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enet_weighing(df, c=1.02):\n",
    "    \"\"\"Computes class weights as described in the ENet paper:\n",
    "\n",
    "        w_class = 1 / (ln(c + p_class)),\n",
    "\n",
    "    where c is usually 1.02 and p_class is the propensity score of that\n",
    "    class:\n",
    "\n",
    "        propensity_score = freq_class / total_pixels.\n",
    "\n",
    "    References: https://arxiv.org/abs/1606.02147\n",
    "\n",
    "    Arguments:\n",
    "        df (pandas.DataFrame): data frame\n",
    "        c (int, optional): hyper-parameter which restricts\n",
    "            the interval of values of the weights. Default: 1.02.\n",
    "\n",
    "    \"\"\"\n",
    "    num_images = len(df[\"ImageId\"].unique())\n",
    "    rle_series = df[\"EncodedPixels\"]    \n",
    "    \n",
    "    total = 768 * 768 * num_images\n",
    "    ship_count = 0\n",
    "    for rle in rle_series:\n",
    "        if not pd.isna(rle):\n",
    "            # Count pixels that belong to ships by summing the length elements in the encoding\n",
    "            rle_split = rle.split()\n",
    "            for value in rle_split[1::2]:\n",
    "                ship_count += int(value)\n",
    "\n",
    "    # Compute propensity score and then the weights for each class\n",
    "    class_count=np.array([total - ship_count, ship_count])\n",
    "    propensity_score = class_count / total\n",
    "    class_weights = 1 / (np.log(c + propensity_score))\n",
    "\n",
    "    return class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1.42340848, 47.8291847 ])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enet_weighing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def median_freq_balancing(df):\n",
    "    \"\"\"Computes class weights using median frequency balancing as described\n",
    "    in https://arxiv.org/abs/1411.4734:\n",
    "\n",
    "        w_class = median_freq / freq_class,\n",
    "\n",
    "    where freq_class is the number of pixels of a given class divided by\n",
    "    the total number of pixels in images where that class is present, and\n",
    "    median_freq is the median of freq_class.\n",
    "\n",
    "    Arguments:\n",
    "        df (pandas.DataFrame): \n",
    "\n",
    "    \"\"\"\n",
    "    num_images = len(df[\"ImageId\"].unique())\n",
    "    rle_series = df[\"EncodedPixels\"]\n",
    "    total = 768 * 768 * num_images  \n",
    "    ship_total = 0\n",
    "    ship_count = 0\n",
    "    for rle in rle_series:\n",
    "        if not pd.isna(rle):\n",
    "            ship_total += 768 * 768\n",
    "            \n",
    "            # Count pixels that belong to ships by summing the length elements in the encoding\n",
    "            rle_split = rle.split()\n",
    "            for value in rle_split[1::2]:\n",
    "                ship_count += int(value)\n",
    "\n",
    "    # Compute the frequency and its median\n",
    "    class_count=np.array([total - ship_count, ship_count])\n",
    "    total = np.array([total, ship_total])\n",
    "    freq = class_count / total\n",
    "    med = np.median(freq)\n",
    "\n",
    "    return med / freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.5013302 , 188.44108246])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "median_freq_balancing(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Airbus",
   "language": "python",
   "name": "airbus-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
